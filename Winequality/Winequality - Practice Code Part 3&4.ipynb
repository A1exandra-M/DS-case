{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Wrangling and Transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b371c22ed026>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# StandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdataset_sc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "# StandardScaler \n",
    "sc = StandardScaler()\n",
    "\n",
    "dataset_sc = sc.fit_transform(dataset)\n",
    "\n",
    "# for under-sampling dataset\n",
    "#dataset_sc = sc.fit_transform(dataset_under)\n",
    "\n",
    "# for over-sampling dataset\n",
    "#dataset_sc = sc.fit_transform(dataset_over)\n",
    "\n",
    "dataset_sc = pd.DataFrame(dataset_sc)\n",
    "dataset_sc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Creating datasets for ML part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 'X' for features' and y' for the target ('quality').\n",
    "y = target\n",
    "X = dataset_sc.copy()\n",
    "\n",
    "# for under-sampling dataset \n",
    "#y = target_under\n",
    "#X = dataset_sc.copy()\n",
    "\n",
    "# for over-sampling dataset \n",
    "#y = target_over\n",
    "#X = dataset_sc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preview of the first 5 lines of the loaded data \n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### 'Train\\Test' split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply 'Train\\Test' splitting method\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print shape of X_train and y_train\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print shape of X_test and y_test\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Build, train and evaluate models without hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logistic Regression\n",
    "* K-Nearest Neighbors\n",
    "* Decision Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "LR_pred = LR.predict(X_test)\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "KNN = KNeighborsClassifier()\n",
    "KNN.fit(X_train, y_train)\n",
    "KNN_pred = KNN.predict(X_test)\n",
    "\n",
    "# Decision Tree\n",
    "DT = DecisionTreeClassifier(random_state = 0)\n",
    "DT.fit(X_train, y_train)\n",
    "DT_pred = DT.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"LR Classification Report: \\n\", classification_report(y_test, LR_pred, digits = 6))\n",
    "print(\"KNN Classification Report: \\n\", classification_report(y_test, KNN_pred, digits = 6))\n",
    "print(\"DT Classification Report: \\n\", classification_report(y_test, DT_pred, digits = 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_confusion_mx = confusion_matrix(y_test, LR_pred)\n",
    "print(\"LR Confusion Matrix: \\n\", LR_confusion_mx)\n",
    "print()\n",
    "KNN_confusion_mx = confusion_matrix(y_test, KNN_pred)\n",
    "print(\"KNN Confusion Matrix: \\n\", KNN_confusion_mx)\n",
    "print()\n",
    "DT_confusion_mx = confusion_matrix(y_test, DT_pred)\n",
    "print(\"DT Confusion Matrix: \\n\", DT_confusion_mx)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### ROC-AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roc_auc_score(DT_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Build, train and evaluate models with hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "LR = LogisticRegression()\n",
    "LR_params = {'C':[1,2,3,4,5,6,7,8,9,10], 'penalty':['l1', 'l2', 'elasticnet', 'none'], 'solver':['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'], 'random_state':[0]}\n",
    "LR1 = GridSearchCV(LR, param_grid = LR_params)\n",
    "LR1.fit(X_train, y_train)\n",
    "LR1_pred = LR1.predict(X_test)\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "KNN = KNeighborsClassifier()\n",
    "KNN_params = {'n_neighbors':[5,7,9,11]}\n",
    "KNN1 = GridSearchCV(KNN, param_grid = KNN_params)             \n",
    "KNN1.fit(X_train, y_train)\n",
    "KNN1_pred = KNN1.predict(X_test)\n",
    "\n",
    "# Decision Tree\n",
    "DT = DecisionTreeClassifier()\n",
    "DT_params = {'max_depth':[2,10,15,20], 'criterion':['gini', 'entropy'], 'random_state':[0]}\n",
    "DT1 = GridSearchCV(DT, param_grid = DT_params)\n",
    "DT1.fit(X_train, y_train)\n",
    "DT1_pred = DT1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the best hyper parameters set\n",
    "print(\"Logistic Regression Best Hyper Parameters:   \", LR1.best_params_)\n",
    "print(\"K-Nearest Neighbour Best Hyper Parameters:   \", KNN1.best_params_)\n",
    "print(\"Decision Tree Best Hyper Parameters:         \", DT1.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LR Classification Report: \\n\", classification_report(y_test, LR1_pred, digits = 6))\n",
    "print(\"KNN Classification Report: \\n\", classification_report(y_test, KNN1_pred, digits = 6))\n",
    "print(\"DT Classification Report: \\n\", classification_report(y_test, DT1_pred, digits = 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix of DT model\n",
    "DT_confusion_mx = confusion_matrix(y_test, DT1_pred)\n",
    "print('DT Confusion Matrix')\n",
    "\n",
    "# visualisation\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(DT_confusion_mx, annot = True, fmt = 'd', cmap = 'Blues', ax = ax, linewidths = 0.5, annot_kws = {'size': 15})\n",
    "ax.set_ylabel('FP       True label        TP')\n",
    "ax.set_xlabel('FN       Predicted label        TN')\n",
    "ax.xaxis.set_ticklabels(['1', '0'], fontsize = 10)\n",
    "ax.yaxis.set_ticklabels(['1', '0'], fontsize = 10)\n",
    "plt.show()\n",
    "print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### ROC-AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(DT1_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission of .csv file with predictions\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = X_test.index\n",
    "sub['quality'] = DT1_pred\n",
    "sub.to_csv('WinePredictionsTest.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Predict which wines are 'Good/1' and 'Not Good/0' (use binary classification; check balance of classes; calculate perdictions; choose the best model).\n",
    "\n",
    "**Answers**:\n",
    "\n",
    "1. Binary classification was applied.\n",
    "\n",
    "2. Classes were highly imbalanced with 78.36 % of '0' class and only 21.64 % of '1' class in our dataset. \n",
    "\n",
    "3. Three options were applied in order to calculate the best predictions:\n",
    "    * Calculate predictions with imbalanced dataset\n",
    "    * Calculate predictions with random under-sampling technique of an imbalanced dataset\n",
    "    * Calculate predictions with random over-sampling technique of an imbalanced dataset\n",
    "    \n",
    "4. Three ML models were used: Logistic Regression, KNN, Decision Tree (without and with hyper parameters).\n",
    "\n",
    "5. The best result was choosen: \n",
    "    * Random over-sampling dataset with 3838 enteties in class '0' and 3838 enteties in class '1', 7676 enteties in total.\n",
    "    * Train/Test split: test_size=0.2, random_state=0\n",
    "    * Decision Tree model without hyper parameters tuning, with an accuracy score equal ... and ROC-AUC score equal ... ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
